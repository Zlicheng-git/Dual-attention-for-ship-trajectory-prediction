
import tensorflow as tf
from tensorflow import keras
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import random
from math import sqrt
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"
os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'


trej1 = pd.read_csv(r"G:/data/trej1.csv")
# data_per1 = data_per1.iloc[150:,:]
data_per1 = trej1.copy()
# data_per1['Time'] = data_per['Time']
data_per1.drop(index=data_per1[data_per1["Latitude_N"] >= 62].index.values, axis=0, inplace=True)
data_orig = data_per1.copy()

def Standardization(data):
    return (data - data.mean(axis=0)) / data.std(axis=0)

def RMSE(y_pre, y_real):
    return sqrt(np.sum((y_pre - y_real) ** 2) / y_pre.shape[0])

def loss_function(predicted_x , target ):
    loss = np.sum(np.square(predicted_x - target) , axis= 1)/(predicted_x.size()[1])
    loss = np.sum(loss)/loss.shape[0]
    return loss
def samples(data, num):
    seq_data = pd.DataFrame()
    for k in range(0, (data.shape[0] - num) + 1):
        seq_data = seq_data.append(data[k : (k + num)])
    return seq_data
# 去除不需要的列
def data_columns(data, need_col):
    index = list(data.columns)
    drop_col = list(set(index) - set(need_col))
    data.drop(drop_col, inplace=True, axis=1)
    data.drop(index=data[data["Latitude_N"] >= 62].index.values, axis=0, inplace=True)
    data.reset_index(drop=True, inplace=True)
    return data

def predict_y_samples(data, pre_k, num):
    data = Standardization(data)
    for i in range(pre_k):
        data.loc[data.iloc[(num-1):-(i+1), :].index, 'per_long_'+str(i+1)] = data.iloc[(num+i):,:].Longitude_W.values
        data.loc[data.iloc[(num-1):-(i+1), :].index, 'per_lat_'+str(i+1)] = data.iloc[(num+i):,:].Latitude_N.values
        
    data.drop(index=data.iloc[data.shape[0]-pre_k:, :].index.values, axis=0, inplace = True)
    return data

def random_point_get_samples(data, num, probability, need_col, pre_k, input_size):
    index = data.index[data.index >= num]
    train_index = random.sample(list(index), int(index.shape[0] * probability))
    # train_index = index[0:int(index.shape[0] * probability)]
    test_index = list(set(index) - set(train_index))
    train_x, train_y, test_x, test_y = pd.DataFrame(), {}, pd.DataFrame(), {}

    for j in range(pre_k):
        train_y[j+1], test_y[j+1] = [], []
    
    for i in train_index:
        train_x = train_x.append(data.loc[int(i-(num-1)):int(i),need_col])
        for j in range(pre_k):
            train_y[j+1].append(data.loc[int(i),['per_long_'+str(j+1), 'per_lat_'+str(j+1)]].values)
            
    for i in test_index:
        test_x = test_x.append(data.loc[int(i-(num-1)):int(i),need_col])
        for j in range(pre_k):
            test_y[j+1].append(data.loc[int(i),['per_long_'+str(j+1), 'per_lat_'+str(j+1)]].values)
    
    return train_index, train_x.values.reshape(-1, num, input_size), test_x.values.reshape(-1, num, input_size), train_y, test_y

def yshape(train_y, test_y):
    ytrain, ytest = {}, {}
    for i in list(train_y.keys()):
        ytrain[i] = np.array(train_y[i])
        ytest[i] = np.array(test_y[i])
        if i <= 1:
            y_train = ytrain[i]
            y_test = ytest[i]
        else:
            y_train = np.c_[y_train, ytrain[i]]
            y_test = np.c_[y_test, ytest[i]]            
    return y_train.reshape(-1,y_train.shape[-1]), y_test.reshape(-1, y_test.shape[-1])


class Encoder(keras.models.Model):
    def __init__(self, units=128):
        """ attent_size:自注意力机制的维度; units: Lstm 输出维度 """
        super().__init__()
        self.attention = keras.layers.Attention()
        self.lstm = keras.layers.LSTM(units, return_sequences=True, return_state=True
                                       # , activity_regularizer=tf.keras.regularizers.l2(0.05) 
                                        , kernel_regularizer=tf.keras.regularizers.l2(0.003)
                                      ) 
        self.lstm1 = keras.layers.LSTM(units, return_sequences=True, return_state=True
                                       # , activity_regularizer=tf.keras.regularizers.l2(0.05) 
                                        , kernel_regularizer=tf.keras.regularizers.l2(0.003)
                                      ) 
        self.dropout = keras.layers.Dropout(rate=0.3)

        
    def call(self, inputs):
        encoder_output, state_h1, state_c1 = self.lstm(inputs)
        # self.dropout(encoder_output)
        encoder_output, state_h2, state_c2 = self.lstm1(encoder_output)
        # print(encoder_output.shape)
        # print(dense.shape)
        return encoder_output, [state_h1, state_c1], [state_h2, state_c2] #[state_h, state_c]


class Decoder(keras.models.Model):
    def __init__(self, units = 128, label = 1, pre_k=None):
        """ units: Lstm 输出维度; label:最后输出的结果的维度 """
        super().__init__()
        self.pre_k = pre_k
        self.lstm = keras.layers.LSTMCell(units # , return_sequences=True, return_state=True
                                    , activity_regularizer=tf.keras.regularizers.l2(0.003)
                                    # , kernel_regularizer=tf.keras.regularizers.l2(0.05)
                                      )
        self.lstm1 = keras.layers.LSTMCell(units # , return_sequences=True, return_state=True 
                                        , activity_regularizer=tf.keras.regularizers.l2(0.003)
                                        # , kernel_regularizer=tf.keras.regularizers.l2(0.05)
                                      )

        self.attention = keras.layers.Attention()
        self.MultiHeadAttention = tf.keras.layers.MultiHeadAttention(num_heads=3, key_dim=2)
        self.dropout = keras.layers.Dropout(rate=0.3)
        self.dense = keras.layers.Dense(label) 
        self.dense1 = keras.layers.Dense(label) 
        self.dense2 = keras.layers.Dense(units)
        self.reshape = keras.layers.Reshape((1,-1))
        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)


    def call(self, inputs, encoder_state1, encoder_state2, encoder_output):
        att_values, weights = self.MultiHeadAttention(inputs, inputs, return_attention_scores=True)
        att_values = self.layernorm(inputs + att_values)
        # att_values = tf.add(inputs, att_values)
        att_values = tf.reshape(att_values, shape=[-1, att_values.shape[-1] * att_values.shape[-2]])
        shape = int(inputs.shape[1] * inputs.shape[2])
        inputs = keras.layers.Reshape([shape])(inputs) 
                
        decoder_output2, state_h2 = self.lstm(inputs, states=encoder_state1)
        decoder_output, state_h = self.lstm1(decoder_output2, states=encoder_state2)
        decoder_output1 = self.reshape(decoder_output)
        attention_output, attention_scores1 = self.attention([decoder_output1, encoder_output], return_attention_scores=True)
                
        att_values = self.dense2(att_values)
        attention_output = tf.reshape(attention_output, shape=[-1, attention_output.shape[-1]])
        contanct_input = keras.layers.concatenate([attention_output, decoder_output, att_values], axis=1)
        print(contanct_input.shape,'+++')

        output = self.dense(contanct_input)
        output2 = self.dense1(contanct_input)
        print(output.shape)
 
        return output, output2, [weights, attention_scores1]

num = 5
time_step = num
pre_k = 1
epochs = 2000
batch_size = 512
need_col = ['Longitude_W', 'Latitude_N', 'speed_knots','trueHeading', 'windAngle', 'windSpeed'] 
data_per1 = data_columns(data_per1, need_col=need_col)
input_size = int(len(need_col))
data_per1 = predict_y_samples(data_per1, pre_k=pre_k, num=num)
datz= data_per1.copy()
train_index, train_x, test_x, train_y, test_y = random_point_get_samples(data_per1, input_size=input_size
                                                                         , num=num
                                                                         , probability=0.6
                                                                         , need_col=need_col, pre_k=pre_k)
y_train, y_test = yshape(train_y, test_y)
            
encoder_input = keras.layers.Input(shape=([time_step, train_x.shape[2]]))
encoder_output,encoder_state1, encoder_state2=Encoder()(encoder_input)
            
output_long, output_lat, weights = Decoder(pre_k=pre_k)(encoder_input, encoder_state1, encoder_state2, encoder_output) #_long, output_lat

model = keras.models.Model([encoder_input], [output_long, output_lat])
# model.summary()
model.compile(loss = keras.losses.MeanSquaredError()
                          ,optimizer = keras.optimizers.Adam(learning_rate=0.01) 
                          ,metrics=keras.metrics.RootMeanSquaredError()) #MeanAbsoluteError()
history = model.fit([train_x], [y_train[:,0], y_train[:,1]], epochs=epochs
                                , batch_size = batch_size
                                , validation_data=([test_x], [y_test[:,0], y_test[:,1]]))
score_ourmodel = model.evaluate([test_x], [y_test[:,0], y_test[:,1]])

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.xlabel('Epochs %s batch_size %s' % (epochs, batch_size), fontsize = 12)
plt.ylabel('MSE', fontsize = 12)
plt.show()
plt.plot(history.history[list(history.history.keys())[-6]], label='train')
plt.plot(history.history[list(history.history.keys())[-1]], label='test')
plt.legend()
plt.xlabel('Epochs %s batch_size %s' % (epochs, batch_size), fontsize = 12)
plt.ylabel('RMSE', fontsize = 12)
plt.show()
plt.plot(history.history[list(history.history.keys())[-7]], label='train')
plt.plot(history.history[list(history.history.keys())[-2]], label='test')
plt.legend()
plt.xlabel('Epochs %s batch_size %s' % (epochs, batch_size), fontsize = 12)
plt.ylabel('RMSE', fontsize = 12)
plt.show()










